{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Exercise session 09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise session we will focus on neural networks (NN), and convolutional neural networks (CNN).\n",
    "\n",
    "The first two problems deal with dense NN and must be submitted for grading.\n",
    "\n",
    "The third problem deals with CNN and it is **not** counted for grading.\n",
    "\n",
    "The fourth problem shows additional topics about NN and it is **not** counted for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of dense neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have `tensorflow` installed on your computer. `Tensorflow` is a software geared towards Deep Learning, i.e., neural networks. It comes as a Python package, so it must be installed as you would install other packages, such as `numpy`, `pandas`, and `scikit-learn`.\n",
    "`Keras` is a library that comes with `tensorflow` and allows you to create, fit, and predict neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:05.480327Z",
     "start_time": "2021-04-17T12:11:03.434226Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0d86e9299a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this problem is to build an image classifier with a dense NN. We will be using the Fashion MNIST dataset [https://github.com/zalandoresearch/fashion-mnist#readme].\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples.\n",
    "The dataset consists of 10 classes which are `T-shirt/top`, `Trouser`, `Pullover`, `Dress`, `Coat`, `Sandal`, `Shirt`, `Sneaker`, `Bag`, and `Ankle boot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the data by filling the `??`. Furthermore, investigate the dimension and type of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(??, ??), (??, ??) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The pixel intensities are represented as integers (from 0 to 255). Since we are going to train the NN using gradient descent, we must scale the input features so that they lie in the [0, 1] interval.\n",
    "Fill in the `??`.\n",
    "\n",
    "_Hint_: Notice that you want your input features to be floats, but the original values are stored as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = ?? / ??\n",
    "X_train_scaled.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the dataset has no validation set, let us create one. Split the training set so that you have 5,000 observations in the validation set. Fill in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = X_train_scaled[:??]\n",
    "X_train = X_train_scaled[??:]\n",
    "y_valid = y_train_full[:??]\n",
    "y_train = y_train_full[??:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the first image in the training dataset using the `plt.imshow` function. Set the argument `cmap=\"binary\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this dataset, the classes are encoded as integers from 0 to 9. From the dataset websites, retrieve the names of the classes and store them in a list of strings. Fill in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [??, ??, ??, ??, ??, ??, ??, ??, ??, ??]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Having encoded the integer classes with their names, we can now plot some images and their corresponding labels. Fill in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(??):\n",
    "    for col in range(??):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[??], cmap=\"binary\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are now ready to build the architecture for our neural network. We want two hidden layers with 300 and 100 nodes respectively and the `\"relu\"` activation function. For the final layer we use the `\"softmax\"` activation function. Why do we need to \"flatten\" the inputs? Fill in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[??,??]),\n",
    "    keras.layers.Dense(units=??, activation=??),\n",
    "    keras.layers.Dense(units=??, activation=??),\n",
    "    keras.layers.Dense(units=??,  activation=??)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the `summary()` method of the object `model`, try to understand how to compute the number of parameters.\n",
    "\n",
    "_Hint_: do not forget about the bias terms (i.e., intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the `get_layer` and `get_weights` methods, you can access the values of the weights of each layer. Fill in the `??`. Why do you think the weights are not initialized to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.get_layer(??).get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that the model is created, we must compile it. Specify as loss `\"sparse_categorical_crossentropy\"`, as optimizer `\"sgd\"`, and as metrics `\"accuracy\"`. Fill in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = ??,\n",
    "              optimizer = ??\n",
    "              metrics= [??])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, let us train the model over 30 epochs. The stochastic gradient descent algorithm handles one mini-batch of observations at a time (e.g., 32 observations), and it goes through the whole training set. Each pass is called an _epoch_. Fill in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=??, y=??, \n",
    "                   validation_data=(??, ??),\n",
    "                   epochs=??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that our model is trained, let us plot the learning curves. Fill in the `??`. Is there any evidence of overfitting? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(??)\n",
    "plt.gca().set_ylim(??, ??);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us evaluate our model on the test set by calling the `evaluate()` method of the `model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us predict the class of the observations 24, 188, 3023 in the test set and plot the results. Fill in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [??]\n",
    "X_new = X_test[??]\n",
    "y_pred = np.argmax(model.predict(??), axis = 1)\n",
    "y_new = y_test[??]\n",
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"True classes:\", y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    cls_nm = class_names[??]\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis('off')\n",
    "    plt.title(cls_nm, fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced topics in dense neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the example of the previous problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and restoring fitted models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since training neural networks is computationally intensive and time consuming, it is important to be able to save and restore trained models. Keras makes this very easy. Save and reload the model fitted in the previous exercise by filling in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(??)\n",
    "model_restored = keras.models.load_model(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training, to avoid losing everything if your computer crashes. You can do so by using the `callbacks` argument in the `fit()` function. This argument accepts list of `callbacks`. A `callback` is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc) (https://keras.io/api/callbacks). Fill in the `??` to save the checkpoints of your model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(??)\n",
    "history = model.fit(??, ??, \n",
    "                    validation_data=(??, ??), \n",
    "                    epochs=??, callbacks=[??])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A useful strategy to avoid overfitting with neural networks is by using early stopping, i.e., interrupt the training when there is no improvement on the validation set for a number of epochs. When this happens, early stop allows you to roll back to the best model. Notice that early stop is also implemented as a `Keras` callback. Fill in the `??`.\n",
    "\n",
    "_Hint_: The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no\n",
    "need to restore the best model saved because the callback will keep track of the best weights and restore them for you at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=??, restore_best_weights=??)\n",
    "\n",
    "history = model.fit(??, ??,\n",
    "                   validation_data=(??, ??), \n",
    "                   epochs=??, callbacks=[??])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In neural networks there is high risk of overfitting the training data. Therefore, it is important to apply some form of regularization. A very popular one is the `dropout` which consists of dropping randomly a given percentage of nodes in some layers (during **training only**). To implement dropout using Keras, you can use the `keras.layers.Dropout` layer. Fill in the `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[??,??]),\n",
    "    keras.layers.Dense(units=??, activation=??),\n",
    "    keras.layers.Dense(units=??, activation=??),\n",
    "    keras.layers.Dropout(rate=??),\n",
    "    keras.layers.Dense(units=??,  activation=??)\n",
    "])\n",
    "\n",
    "model.compile(loss = ??,\n",
    "              optimizer = ??,\n",
    "              metrics= [??])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(??, ??,\n",
    "                   validation_data=(??, ??), \n",
    "                   epochs=??, callbacks=[??])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(??, ??);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finding a good learning rate is very important. If you set it much too high, training may diverge (as we discussed in “Gradient Descent”). If you set it too low, training will eventually converge to the optimum, but it will take a very long time. One of the easiest options is to use a constant learning rate. Fill in the `??` by setting the learning rate to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=??)\n",
    "model.compile(loss = ??,\n",
    "              optimizer = optimizer,\n",
    "              metrics= [??])\n",
    "history = model.fit(??, ??,\n",
    "                   validation_data=(??, ??), \n",
    "                   epochs=??, callbacks=[??])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional neural networks\n",
    "\n",
    "##### This problem is not graded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we show how to create and train a convolutional neural network to the fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We first have to adjust the predictor dataset by adding a third dimension. In CNN, each image is encoded as with three dimension (height, width, depth), where depth represents the color scales. Black and white images only have the scale of greys, so depth = 1. Color images, have the red, green, and blue scales (RGB), so depth = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We begin by building the CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(\n",
    "    keras.layers.Conv2D,\n",
    "    filters = 64, # number of filters\n",
    "    kernel_size = (3, 3), # height and width of each filter)\n",
    "    padding = \"same\", # when strides = 1, input and output layers have same dimensions\n",
    "    activation = \"relu\")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "     DefaultConv2D(filters = 64, \n",
    "                   kernel_size = (7, 7), # height and width of each filter)\n",
    "                   input_shape = [28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We compile the model by defining the loss, optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", # alternative to SGD\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We train the model saving the results in variable named `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x = X_train, \n",
    "          y = y_train,\n",
    "          epochs = 10,\n",
    "          validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us evaluate the model performance and make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.arange(1, 11)\n",
    "X_new = X_test[indexes]\n",
    "y_pred = np.argmax(model.predict(X_new), axis = 1)\n",
    "y_new = y_test[indexes]\n",
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"True classes     :\", y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Additional topics\n",
    "\n",
    "##### This problem is not graded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does `model.compile` initialize all the weights?\n",
    "Notice that once you create a model with `keras.models.Sequential` for example, all the weights are initialized.\n",
    "When you compile a model with `model.compile` you just set:\n",
    "- a loss function,\n",
    "- an optimizer,\n",
    "- some metrics,\n",
    "- possibly, some callbacks function.\n",
    "\n",
    "If you have a model that you already trained and you wish, for example, to change the learning rate of the optimizer, you can just compile the model again. Nothing happens to the pre-existing weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning neural networks\n",
    "Neural networks have many hyperparameters, such as the number of hidden layers, nodes per hidden layer, learning rate, dropout rate, etc.\n",
    "`Keras Tuner` is a Python package that helps you build a grid of hyperparameter and run a search across these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    '''\n",
    "    Hyperparameters -> Keras compiled model\n",
    "    Produces a Keras compiled model, taking one combination of parameters in hp\n",
    "    '''\n",
    "    \n",
    "    model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(units=300, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "    keras.layers.Dropout(rate=hp.get(\"dropout_rate\")),\n",
    "    keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=hp.get(\"learning_rate\"))\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                  optimizer = \"sgd\",\n",
    "                  metrics= [\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HyperParameters()\n",
    "hp.Choice('learning_rate', [1e-1, 1e-3])\n",
    "hp.Choice('dropout_rate', [0.0, 0.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(hypermodel = build_model,\n",
    "                     hyperparameters=hp,\n",
    "                     max_trials=5, \n",
    "                     objective=\"val_accuracy\", \n",
    "                     allow_new_entries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x=X_train,\n",
    "             y=y_train,\n",
    "             epochs=5,\n",
    "             validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a summary of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model.\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Refit with the best hyperparameters\n",
    "history = best_model.fit(X_train, y_train, \n",
    "                         validation_data=(X_valid, y_valid), \n",
    "                         epochs=30, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "best_model.evaluate(X_test, y_test);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "55px",
    "width": "160px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
