{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cd5ff1",
   "metadata": {},
   "source": [
    "#  Pipelines and composite estimators\n",
    "\n",
    "### Pipeline: chaining estimators\n",
    "\n",
    "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
    "\n",
    "**Convenience and encapsulation**\n",
    "  * You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "  \n",
    "**Joint parameter selection**\n",
    "  * You can grid search over parameters of all estimators in the pipeline at once.\n",
    "  \n",
    "**Safety** \n",
    "  * Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e4e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe\n",
    "Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])\n",
    "\n",
    "# Or use `from sklearn.pipeline import make_pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a913d",
   "metadata": {},
   "source": [
    "### Nested parameters\n",
    "Parameters of the estimators in the pipeline can be accessed using the \\<estimator\\>__\\<parameter\\> syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(clf__C=10)  # Set C in SVC to 10\n",
    "\n",
    "# Especially useful with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = dict(reduce_dim__n_components=[2, 5, 10],clf__C=[0.1, 10, 100])\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4e05c",
   "metadata": {},
   "source": [
    "### Transforming target in regression\n",
    " \n",
    "TransformedTargetRegressor transforms the targets y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07052d",
   "metadata": {},
   "source": [
    "### FeatureUnion: composite feature spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e4efc",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Data preprocessing is the process of transforming raw data into an understandable format. It is also an important step in data mining as we cannot work with raw data. The quality of the data should be checked before applying machine learning or data mining algorithms.\n",
    "\n",
    "## Why is Data preprocessing important?\n",
    "\n",
    "Preprocessing of data is mainly to check the data quality. The quality can be checked by the following\n",
    "\n",
    "   * Accuracy: To check whether the data entered is correct or not.\n",
    "   * Completeness: To check whether the data is available or not recorded.\n",
    "   * Consistency: To check whether the same data is kept in all the places that do or do not match.\n",
    "   * Timeliness: The data should be updated correctly.\n",
    "   * Believability: The data should be trustable.\n",
    "   * Interpretability: The understandability of the data.\n",
    "\n",
    "**Major Tasks in Data Preprocessing:**\n",
    "   1. Data cleaning\n",
    "   2. Data integration\n",
    "   3. Data reduction\n",
    "   4. Data transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278f221",
   "metadata": {},
   "source": [
    "## 1. Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ab9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Uniform distribution, merely rescale with variant and shift to zero\n",
    "uniform = np.random.randint(-5,21,size=100000)\n",
    "uniform = uniform.reshape(-1,1).astype(float)\n",
    "scaler = preprocessing.StandardScaler().fit(uniform)\n",
    "\n",
    "uniform_scaled = scaler.transform(uniform)\n",
    "\n",
    "plt.hist(uniform.reshape(-1), bins=np.arange(-10,24,1))\n",
    "plt.show()\n",
    "plt.hist(uniform_scaled.reshape(-1), bins=np.arange(-10,11,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max range scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaler = min_max_scaler.fit(uniform)\n",
    "\n",
    "min_max_scaled = scaler.transform(uniform)\n",
    "plt.hist(uniform.reshape(-1), bins=np.arange(-10,24,1))\n",
    "plt.show()\n",
    "plt.hist(min_max_scaled.reshape(-1), bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling sparse data\n",
    "# Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales.\n",
    "# MaxAbsScaler was specifically designed for scaling sparse data, and is the recommended way to go about this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c384361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data with outliers\n",
    "# Scaling using the mean and variance of the data is likely to not work very well if there are many outliners. \n",
    "# In these cases, you can use RobustScaler as a drop-in replacement instead. \n",
    "# It uses more robust estimates for the center and range of your data.\n",
    "\n",
    "# robust scaler\n",
    "narrow_norm = np.random.normal(0,1,size=1000)\n",
    "wide_uniform = np.random.randint(0,1000,size=10)\n",
    "combine = np.concatenate((narrow_norm,wide_uniform))\n",
    "combine = combine.reshape(-1,1)\n",
    "\n",
    "robustscaler = preprocessing.RobustScaler()\n",
    "scaler = robustscaler.fit(combine)\n",
    "\n",
    "robust_scaled = scaler.transform(combine)\n",
    "plt.hist(combine.reshape(-1), bins='auto')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.hist(robust_scaled.reshape(-1), bins='auto')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "# standardScaler\n",
    "scaler = preprocessing.StandardScaler().fit(combine)\n",
    "standard_scaled = scaler.transform(combine)\n",
    "\n",
    "plt.hist(standard_scaled.reshape(-1), bins='auto')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cf6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering kernel matrices\n",
    "\n",
    "# To be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3125ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('Erdos': conda)",
   "language": "python",
   "name": "python371064biterdosconda7530abd590984c45ba6372cad090cba4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
